{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo completo de un Perceptron en Pytorch\n",
    "# Se observa el gradiente y las actualizaciones de los pesos\n",
    "# Implementación elemental de Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1: elemental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2352e-08],\n",
      "        [ 4.0000e-01],\n",
      "        [ 8.0000e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Defino una matriz X (3,2) = (batch_size, input_size=no_features)\n",
    "X = torch.tensor([[1,2], [3,4], [5,6]], dtype=torch.float)\n",
    "\n",
    "# X = \t[1,2\n",
    "#\t\t 3,4\n",
    "#\t\t 5,6]\n",
    "\n",
    "# Defino W y B para mi layer. \n",
    "# W es una matriz (2,1) y B es un vector (1)\n",
    "# (2,1) = (input_size, output_size=n_neuronas)\n",
    "# (1) = (output_size=n_neuronas)\n",
    "\n",
    "W = torch.tensor([[0.5], [-0.3]])\n",
    "B = torch.tensor([0.1]) # pytorch hace la suma element-wise automaticamente\n",
    "\n",
    "# W = \t[0.5\n",
    "#\t\t-0.3]\n",
    "\n",
    "# B = \t[0.1]\n",
    "\n",
    "# Lo que hace el proceso es logit = X*W + B\n",
    "\n",
    "logit = torch.matmul(X, W) + B\n",
    "print(logit)\n",
    "\n",
    "#logit = [0\n",
    "#\t\t  0.4\n",
    "#\t\t  0.8]\n",
    "# estos son, para el batch, los valores predichos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2352e-08],\n",
       "        [ 4.0000e-01],\n",
       "        [ 8.0000e-01]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check in pytorch\n",
    "linear\t= nn.Linear(2, 1) # (input_size, output_size)\n",
    "linear.weight.data = W.T\n",
    "linear.bias.data = B\n",
    "\n",
    "linear(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2: calculo de perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino una matriz X (3,2) = (batch_size, input_size=no_features)\n",
    "X = torch.tensor([[1,2], [3,4], [5,6]], dtype=torch.float)\n",
    "\n",
    "# X = \t[1,2\n",
    "#\t\t 3,4\n",
    "#\t\t 5,6]\n",
    "\n",
    "# Defino W y B para mi layer. \n",
    "# W es una matriz (2,1) y B es un vector (1)\n",
    "# (2,1) = (input_size, output_size=n_neuronas)\n",
    "# (1) = (output_size=n_neuronas)\n",
    "\n",
    "W = torch.tensor([[0.5], [-0.3]])\n",
    "B = torch.tensor([0.1]) # pytorch hace la suma element-wise automaticamente\n",
    "\n",
    "# W = \t[0.5\n",
    "#\t\t-0.3]\n",
    "\n",
    "# B = \t[0.1]\n",
    "\n",
    "y = torch.tensor([[1], [0], [1]], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2352e-08],\n",
      "        [ 4.0000e-01],\n",
      "        [ 8.0000e-01]])\n"
     ]
    }
   ],
   "source": [
    "# 1. Construyo señal neta (=logit).\n",
    "# Lo que hace el proceso es Y_est = X*W + B\n",
    "# El resultado son logits, valores brutos entre [-inf, inf]\n",
    "logits = torch.matmul(X, W) + B\n",
    "print(logits)\n",
    "\n",
    "#sn = [0\n",
    "#\t\t  0.4\n",
    "#\t\t  0.8]\n",
    "# estos son, para el batch, los valores predichos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.5987],\n",
       "        [0.6900]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Aplico función de activación\n",
    "y_probs = torch.sigmoid(logits)\n",
    "y_probs\n",
    "\n",
    "# alternativa\n",
    "# s_layer = nn.Sigmoid()\n",
    "# y_probs = s_layer.forward(logits)\n",
    "# y_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.5987],\n",
       "        [0.6900]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Que es lo mismo que hacerlo a mano 1/(1 + exp(-sn))\n",
    "1/(1 + torch.exp(-logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6591)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculo perdida\n",
    "# Se calcula entropia cruzada binaria.\n",
    "# bce se aplica sobre y_probs, no sobre los logits.\n",
    "# Esto es porque aplica sobre valores entre [0,1] y no sobre valores brutos.\n",
    "# Fijarse que este caso donde el batch es mayor a uno, el valor es el promedio\n",
    "# de las entropias cruzadas de cada elemento del batch.\n",
    "\n",
    "bce_manual = -torch.mean(y * torch.log(y_probs) + (1 - y) * torch.log(1 - y_probs))\n",
    "bce_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6591)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ojo BCEWithLogitsLoss aplica ambas funciones, sigmoid y BCE\n",
    "# esto permite aplicarlo directamente sobre los logits.\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn(logits, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 3: comportamiento del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2352e-08],\n",
      "        [ 4.0000e-01],\n",
      "        [ 8.0000e-01]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.6591, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Defino una matriz X (3,2) = (batch_size, input_size=no_features)\n",
    "X = torch.tensor([[1,2], [3,4], [5,6]], dtype=torch.float)\n",
    "y = torch.tensor([[1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "W = torch.tensor([[0.5], [-0.3]])\n",
    "B = torch.tensor([0.1]) # pytorch hace la suma element-wise automaticamente\n",
    "\n",
    "# X = \t[1,2\n",
    "#\t\t 3,4\n",
    "#\t\t 5,6]\n",
    "\n",
    "# Defino W y B para mi layer. \n",
    "# W es una matriz (2,1) y B es un vector (1)\n",
    "# (2,1) = (input_size, output_size=n_neuronas)\n",
    "# (1) = (output_size=n_neuronas)\n",
    "\n",
    "# W = \t[0.5\n",
    "#\t\t-0.3]\n",
    "\n",
    "# B = \t[0.1]\n",
    "\n",
    "# modelo\n",
    "linear\t= nn.Linear(2, 1) # (input_size, output_size)\n",
    "linear.weight.data = W.T\n",
    "linear.bias.data = B\n",
    "\n",
    "# loss y optimizador\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCE\n",
    "optimizer = optim.SGD(linear.parameters(), lr=0.01) # Stochoastic Gradient Descent.\n",
    "\n",
    "# resultado\n",
    "logits = linear(X)\n",
    "print(logits)\n",
    "\n",
    "# perdida\n",
    "loss = loss_fn(logits, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0847, -0.1551]])\n",
      "tensor([-0.0704])\n"
     ]
    }
   ],
   "source": [
    "# gradiente\n",
    "# estos son los valores del gradiente: dL/dW y dL/dB\n",
    "# que se calculan con backpropagation.\n",
    "optimizer.zero_grad() # limpio gradientes\n",
    "loss.backward() # backpropagation\n",
    "\n",
    "print(linear.weight.grad)\n",
    "print(linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0847],\n",
       "        [-0.1551]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculo a mano de gradiente dL/dW\n",
    "# dL/dW = X^T * (sigmoid(logits) - y) / batch_size\n",
    "grad_dl_dw = torch.matmul(X.T, torch.sigmoid(logits) - y) / y.size(0)\n",
    "grad_dl_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0704, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculo a mano de gradiente dL/dB\n",
    "# dL/dB = sum(sigmoid(logits) - y) / batch_size\n",
    "grad_b_w = torch.mean((torch.sigmoid(logits) - y))\n",
    "grad_b_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 4: aplicacion del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5008],\n",
       "        [-0.2984]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.01\n",
    "\n",
    "new_W = linear.weight.data.T -lr*grad_dl_dw\n",
    "new_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1007], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_B = linear.bias.data - lr*grad_b_w\n",
    "new_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5008, -0.2984]])\n",
      "tensor([0.1007])\n"
     ]
    }
   ],
   "source": [
    "optimizer.step() # actualizo W y B\n",
    "print(linear.weight.data)\n",
    "print(linear.bias.data)\n",
    "\n",
    "# originales\n",
    "# W = torch.tensor([[0.5], [-0.3]])\n",
    "# B = torch.tensor([0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 5: implementación completa en Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X in (3,2), y in (3,1)\n",
    "X = torch.tensor([[1,2], [3,4], [5,6]], dtype=torch.float)\n",
    "y = torch.tensor([[1], [0], [1]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "\n",
    "# Init,loss and optimizer\n",
    "model = Perceptron()\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.1722\n",
      "Epoch 2, Loss: 2.0730\n",
      "Epoch 3, Loss: 1.9755\n",
      "Epoch 4, Loss: 1.8799\n",
      "Epoch 5, Loss: 1.7866\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X)\n",
    "    loss_values = loss(logits, y)\n",
    "    loss_values.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss_values.item():.4f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
